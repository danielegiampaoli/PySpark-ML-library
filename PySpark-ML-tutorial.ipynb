{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_I3Wbv-xd6sB"
      },
      "source": [
        "## **PySpark Machine Learning library tutorial**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXqbUHS0d6sT"
      },
      "source": [
        "### P1. Presentation of the chosen topic\n",
        "\n",
        "This notebook aims at presenting how to compute basic statistical analysis, e.g. summary statistics, contingency tables, correlation matrices, feature selection, and solve typical machine learning problems, i.e. regression, classification and clustering in the PySpark environment using (some) spark.sql, datasets and (mostly) the spark.ml library.\n",
        "\n",
        "Spark provides two libraries for ML, i.e. spark.mllib and spark.ml. The former is based on RDDs and is the oldest one, which is now in maintenance mode though still supported in the latter (and newer) version, which is based on DataFrames and has currently been the primary ML library in Spark since version 2.0, also thanks to its interoperability with SQL/DataFrame queries. Exercises in the following paragraphs are based on spark.ml.\n",
        "\n",
        "Performing machine learning tasks in Spark is much quicker than using standard libraries and sequential programming. Especially in a cluster environment, allowing the system to process data parallelly speeds up the computations a lot, as can be seen in the solution timings provided in the final paragraph of this document. \n",
        "\n",
        "A complete list of classes and functions available in spark.ml, with some useful examples, can be found here: https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html. Some of the most classical ML methods will be presented in this project.\n",
        "\n",
        "### P2. Programming questions\n",
        "\n",
        "Given the *bank.csv* (https://www.kaggle.com/rouseguy/bankbalanced/data) dataset,\n",
        "1. Read and explore the dataset, describe quantitative and qualitative variables (using DataFrames and spark.sql).\n",
        "2. Compute the correlation matrix using spark.ml.\n",
        "3. Perform a feature selection in spark.ml using PCA and show results (new computed features, explained variance). Choose an appropriate number of features. Each column in the dataset must have mean = 0 and std = 1 (standardised columns) before applying PCA.\n",
        "4. Perform a linear regression in spark.ml to estimate the values of the 'pdays' variable using all (or part of) the other quantitative variables in the dataset. Split the dataset into training and test sets, show estimated coefficients, RMSE and R2 indices (or some equivalent).\n",
        "5. Perform a logistic regression in spark.ml to classify records by their 'marital' variable, using all (or part of) the other quantitative variables. Compute area under ROC and accuracy score on both training and test sets.\n",
        "6. Use a Support Vector Machine in spark.ml to perform a binary classification on the 'deposit' attribute. Compute area under ROC and accuracy score on both training and test sets.\n",
        "7. Use k-Means in spark.ml to perform a clustering operation. Use the silhouette score (or some equivalent) to select the most appropriate number of clusters to build.\n",
        "8. Choose one of the methods used in the previous questions and repeat the regression/classification procedure but this time also perform Cross Validation on the training set to select the best hyperparameters. Run the best model on the test set and show results (may be pretty slow).\n",
        "\n",
        "For questions strictly related to machine learning, i.e. questions 4,5,6,7 and 8, try and apply the following pipeline: prepare the data (use VectorAssembler and StringIndexer when appropriate, see below), split the data into training and test sets, build the model, fit the training data to the model, evaluate it on both the training and test sets. If needed, go back and adjust hyperparameters.\n",
        "\n",
        "### P3. Educational goals\n",
        "\n",
        "Spark ML libraries are particularly useful in which they allow to solve ML problems dealing with data at large scale, in a distributed environment, offering a highly scalable approach. In a cluster environment, the details of data distribution and server coordination are hidden to the user, who is left with what looks very similar to sequential programming - not much different, in this respect, from building ML pipelines with SKLearn or Keras. However, Spark has proven to be much more efficient in terms of solution time and its approach is therefore worth exploring. The methods proposed in the questions above here cover most of a basic ML course material and could possibly support lab activities in this respect. Pipelines are pretty standard ad consist of data preparation, splitting, training and evaluation. With hopefully little effort, the ML methods covered here can be adapted to other available algorithms in spark.ml - of which the documentation provides a long list.\n",
        "\n",
        "### Practical note on the usage of spark.ml\n",
        "\n",
        "The one less intuitive aspect about preparing data to fit ML models in Spark is having to first pass predictor (numerical) variables to a VectorAssembler object, which puts them all in a single Vector of features. In the case of a classification problem, also the target variable needs to be passed through a StringIndexer object, which transforms it to a label format (0/1, 0/1/2, ... especially if it comes as a string, e.g. yes/no, male/female/other, ...) as follows:"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "FA_KNsDCd6sc"
      },
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "... have data in a spark df and one column as a target variable ...\n",
        "\n",
        "# turn target string variable to label format\n",
        "label_stringIdx = StringIndexer(inputCol = 'target variable name', outputCol = 'label')\n",
        "model = label_stringIdx.fit(df) # fit df to the new format ('label' + other columns)\n",
        "df1 = model.transform(df) # trigger the transformation\n",
        "\n",
        "# turn numerical variables to feature vector\n",
        "assembler = VectorAssembler(inputCols=[list of numerical variables], outputCol = 'features')\n",
        "input_df = assembler.transform(df1).select('label', 'features') # df now has ('label' + 'features' columns)\n",
        "input_df.show(5, truncate = False)\n",
        "\n",
        "... now input_df can be passed to the model (e.g. log_regression.fit(input_df)) ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whJhd3fEd6se"
      },
      "source": [
        "The StringIndexer step is not required if the target variable is already in label format. In the case of a regression problem, there is no need to transform the target variable, which can be used as it is, but numerical variables still need transforming:"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "bX9eHHxTd6sf"
      },
      "source": [
        "# skip the StringIndexer block\n",
        "assembler = VectorAssembler(inputCols=[list of numerical variables], outputCol = 'features')\n",
        "input_df = assembler.transform(df).select('target variable name', 'features')\n",
        "input_df.show(5, truncate = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR3o_Qn1d6sg"
      },
      "source": [
        "### P4. Proposed solutions\n",
        "\n",
        "I propose a solution for the 8 programming questions and timings obtained. I tried using always the same approach while answering the questions, i.e. preparing the dataset, splitting it into training and test sets, creating the model with some chosen parameters, fitting it to the training set and evaluating it on both the training and test sets. Sometimes I will use NumPy and Pandas libraries (or RDDs) for better handling of the results - sometimes left just as comments - because spark.ml results often come in formats which cannot be used in related contexts in a straightforward way, like matplotlib graphic visualisations. However, I will try to stick to mostly Spark programming, which lets the user solve a ML task from start to finish anyway. Timings only refer to the execution of Spark programming. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEvobJRjd6si"
      },
      "source": [
        "### Solutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftuFvACfd6sk"
      },
      "source": [
        "### Q0. Useful libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-E5Oq1BmBDE"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Fundamentals\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as f\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Vector Assembler and String Indexer\n",
        "#from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Correlation\n",
        "from pyspark.ml.stat import Correlation\n",
        "\n",
        "# PCA\n",
        "from pyspark.ml.feature import PCA\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "\n",
        "# Linear Regression\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Logistic Regression and classification evaluators\n",
        "from pyspark.ml.classification import LogisticRegression # default index rmse\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator # default metrics area under roc, does not support accuracy\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator # default metrics accuracy (also good for binary classification)\n",
        "\n",
        "# Linear SVC\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "\n",
        "# KMeans\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "# Cross Validation\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "# initialise spark session\n",
        "sc = pyspark.SparkContext()\n",
        "spark = SparkSession.builder.appName(\"PySpark Project\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3x311pyj3zd"
      },
      "source": [
        "### Q1. Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbheZEwJj2Xm"
      },
      "outputs": [],
      "source": [
        "# Read the dataset and print some info\n",
        "\n",
        "start = time.time()\n",
        "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"Datasets/bank/bank.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcmXj98Vd6sq",
        "outputId": "baa6ac52-02d5-491b-8416-deac25abd789"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- age: integer (nullable = true)\n",
            " |-- job: string (nullable = true)\n",
            " |-- marital: string (nullable = true)\n",
            " |-- education: string (nullable = true)\n",
            " |-- default: string (nullable = true)\n",
            " |-- balance: integer (nullable = true)\n",
            " |-- housing: string (nullable = true)\n",
            " |-- loan: string (nullable = true)\n",
            " |-- contact: string (nullable = true)\n",
            " |-- day: integer (nullable = true)\n",
            " |-- month: string (nullable = true)\n",
            " |-- duration: integer (nullable = true)\n",
            " |-- campaign: integer (nullable = true)\n",
            " |-- pdays: integer (nullable = true)\n",
            " |-- previous: integer (nullable = true)\n",
            " |-- poutcome: string (nullable = true)\n",
            " |-- deposit: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnSiqpEbd6sr",
        "outputId": "38dd46b3-5821-477f-8ba9-3ae5ec7dd0c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----------+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
            "|age|       job|marital|education|default|balance|housing|loan|contact|day|month|duration|campaign|pdays|previous|poutcome|deposit|\n",
            "+---+----------+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
            "| 59|    admin.|married|secondary|     no|   2343|    yes|  no|unknown|  5|  may|    1042|       1|   -1|       0| unknown|    yes|\n",
            "| 56|    admin.|married|secondary|     no|     45|     no|  no|unknown|  5|  may|    1467|       1|   -1|       0| unknown|    yes|\n",
            "| 41|technician|married|secondary|     no|   1270|    yes|  no|unknown|  5|  may|    1389|       1|   -1|       0| unknown|    yes|\n",
            "| 55|  services|married|secondary|     no|   2476|    yes|  no|unknown|  5|  may|     579|       1|   -1|       0| unknown|    yes|\n",
            "| 54|    admin.|married| tertiary|     no|    184|     no|  no|unknown|  5|  may|     673|       2|   -1|       0| unknown|    yes|\n",
            "+---+----------+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oo6rZHLhd6ss",
        "outputId": "03042821-ee3b-4aef-e0c8-8e0f80c3689d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------------------+------------------+------------------+\n",
            "|summary|               age|           balance|          duration|\n",
            "+-------+------------------+------------------+------------------+\n",
            "|  count|             11162|             11162|             11162|\n",
            "|   mean|41.231947679627304|1528.5385235620856|371.99381831213043|\n",
            "| stddev|11.913369192215518| 3225.413325946149|347.12838571630687|\n",
            "|    min|                18|             -6847|                 2|\n",
            "|    max|                95|             81204|              3881|\n",
            "+-------+------------------+------------------+------------------+\n",
            "\n",
            "+-------+------------------+------------------+------------------+\n",
            "|summary|          campaign|             pdays|          previous|\n",
            "+-------+------------------+------------------+------------------+\n",
            "|  count|             11162|             11162|             11162|\n",
            "|   mean| 2.508421429851281| 51.33040673714388|0.8325568894463358|\n",
            "| stddev|2.7220771816614824|108.75828197197717| 2.292007218670508|\n",
            "|    min|                 1|                -1|                 0|\n",
            "|    max|                63|               854|                58|\n",
            "+-------+------------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Summary statistics\n",
        "\n",
        "# quantitative variables\n",
        "df.describe(['age','balance','duration']).show()\n",
        "df.describe(['campaign','pdays','previous']).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgMMA2QLd6st",
        "outputId": "749c930a-9f66-4fc4-bcb4-de7310cb3c1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+------------------+------------------+------------------+\n",
            "|          job|          avg(age)|      avg(balance)|     avg(duration)|\n",
            "+-------------+------------------+------------------+------------------+\n",
            "|   management| 40.21823850350741|1793.6636788776304| 361.4820732657833|\n",
            "|      retired|  65.4241645244216|2417.2506426735217| 389.9935732647815|\n",
            "|      unknown|46.857142857142854|1945.4571428571428|330.37142857142857|\n",
            "|self-employed|39.809876543209874|1865.3728395061728|396.15555555555557|\n",
            "|      student|26.102777777777778|1500.7833333333333| 330.6722222222222|\n",
            "|  blue-collar| 39.50514403292181|1203.9264403292182|394.65895061728395|\n",
            "| entrepreneur| 42.88719512195122|1621.9420731707316| 370.1829268292683|\n",
            "|       admin.|39.374062968515744|1195.8665667166417| 347.9295352323838|\n",
            "|   technician|  39.0016456390565|1556.2945693911136|363.79155238617665|\n",
            "|     services| 38.14192849404117|1081.1711809317444|385.95557963163594|\n",
            "|    housemaid| 47.44525547445255|1366.1605839416059|348.34671532846716|\n",
            "|   unemployed| 40.99719887955182| 1314.719887955182|422.84313725490193|\n",
            "+-------------+------------------+------------------+------------------+\n",
            "\n",
            "+--------+------------------+------------------+-----------------+\n",
            "| marital|          avg(age)|      avg(balance)|    avg(duration)|\n",
            "+--------+------------------+------------------+-----------------+\n",
            "|divorced|47.365042536736276|1371.8352668213456|392.4818252126837|\n",
            "| married|44.533616753267204|1599.9275704613447|361.0637694851205|\n",
            "|  single| 33.01733939738488|1457.2552586696986|384.1955656623081|\n",
            "+--------+------------------+------------------+-----------------+\n",
            "\n",
            "+---------+------------------+------------------+------------------+\n",
            "|education|          avg(age)|      avg(balance)|     avg(duration)|\n",
            "+---------+------------------+------------------+------------------+\n",
            "|  unknown| 45.52313883299799| 1746.605633802817| 346.4486921529175|\n",
            "| tertiary|39.513147194361615|1845.8690702087285| 368.6278124152887|\n",
            "|secondary| 40.08601168736304|1296.4802775748722|373.34495982468957|\n",
            "|  primary|48.220666666666666|1523.0313333333334|383.80333333333334|\n",
            "+---------+------------------+------------------+------------------+\n",
            "\n",
            "+-------+-----------------+------------------+------------------+\n",
            "|housing|         avg(age)|      avg(balance)|     avg(duration)|\n",
            "+-------+-----------------+------------------+------------------+\n",
            "|     no|43.13637136541404|1764.1562659411666| 360.4645468457745|\n",
            "|    yes|39.11115319068358|1266.1511077447453|384.83298617686046|\n",
            "+-------+-----------------+------------------+------------------+\n",
            "\n",
            "+-------+------------------+------------------+-----------------+\n",
            "|deposit|          avg(age)|      avg(balance)|    avg(duration)|\n",
            "+-------+------------------+------------------+-----------------+\n",
            "|     no| 40.83739145240933|1280.2271411544355| 223.130257108803|\n",
            "|    yes|41.670069956513515|1804.2679145396105|537.2945736434109|\n",
            "+-------+------------------+------------------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# qualitative variables\n",
        "\n",
        "df.select('job','age','balance','duration')\\\n",
        "        .groupby('job')\\\n",
        "        .agg(f.mean('age'),f.mean('balance'),f.mean('duration'))\\\n",
        "        .show()\n",
        "\n",
        "df.select('marital','age','balance','duration')\\\n",
        "        .groupby('marital')\\\n",
        "        .agg(f.mean('age'),f.mean('balance'),f.mean('duration'))\\\n",
        "        .show()\n",
        "             \n",
        "df.select('education','age','balance','duration')\\\n",
        "        .groupby('education')\\\n",
        "        .agg(f.mean('age'),f.mean('balance'),f.mean('duration'))\\\n",
        "        .show()\n",
        "             \n",
        "df.select('housing','age','balance','duration')\\\n",
        "        .groupby('housing')\\\n",
        "        .agg(f.mean('age'),f.mean('balance'),f.mean('duration'))\\\n",
        "        .show()\n",
        "\n",
        "df.select('deposit','age','balance','duration')\\\n",
        "        .groupby('deposit')\\\n",
        "        .agg(f.mean('age'),f.mean('balance'),f.mean('duration'))\\\n",
        "        .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJjPo1-7d6su",
        "outputId": "277790a2-e1ab-4720-cd09-de5f4c97108c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+-----+\n",
            "| marital|count|\n",
            "+--------+-----+\n",
            "|divorced| 1293|\n",
            "| married| 6351|\n",
            "|  single| 3518|\n",
            "+--------+-----+\n",
            "\n",
            "+---------+-----+\n",
            "|education|count|\n",
            "+---------+-----+\n",
            "|  unknown|  497|\n",
            "| tertiary| 3689|\n",
            "|secondary| 5476|\n",
            "|  primary| 1500|\n",
            "+---------+-----+\n",
            "\n",
            "+-------+-----+\n",
            "|housing|count|\n",
            "+-------+-----+\n",
            "|     no| 5881|\n",
            "|    yes| 5281|\n",
            "+-------+-----+\n",
            "\n",
            "+-------+-----+\n",
            "|deposit|count|\n",
            "+-------+-----+\n",
            "|     no| 5873|\n",
            "|    yes| 5289|\n",
            "+-------+-----+\n",
            "\n",
            "+---------------+----+----+\n",
            "|marital_deposit|  no| yes|\n",
            "+---------------+----+----+\n",
            "|        married|3596|2755|\n",
            "|         single|1606|1912|\n",
            "|       divorced| 671| 622|\n",
            "+---------------+----+----+\n",
            "\n",
            "+-----------------+----+----+\n",
            "|education_deposit|  no| yes|\n",
            "+-----------------+----+----+\n",
            "|         tertiary|1693|1996|\n",
            "|        secondary|3026|2450|\n",
            "|          primary| 909| 591|\n",
            "|          unknown| 245| 252|\n",
            "+-----------------+----+----+\n",
            "\n",
            "+---------------+----+----+\n",
            "|housing_deposit|  no| yes|\n",
            "+---------------+----+----+\n",
            "|            yes|3346|1935|\n",
            "|             no|2527|3354|\n",
            "+---------------+----+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# qualitative variables\n",
        "\n",
        "#df.groupBy(\"job\").count().show()\n",
        "df.groupBy(\"marital\").count().show()\n",
        "df.groupBy(\"education\").count().show()\n",
        "df.groupBy(\"housing\").count().show()\n",
        "df.groupBy(\"deposit\").count().show()\n",
        "df.crosstab('marital','deposit').show()\n",
        "df.crosstab('education','deposit').show()\n",
        "df.crosstab('housing','deposit').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMtFuV5Nd6sv",
        "outputId": "4dbff99a-f6a5-4f63-fa3b-68c901f71f5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-----+\n",
            "|default|count|\n",
            "+-------+-----+\n",
            "|     no|10994|\n",
            "|    yes|  168|\n",
            "+-------+-----+\n",
            "\n",
            "+----+-----+\n",
            "|loan|count|\n",
            "+----+-----+\n",
            "|  no| 9702|\n",
            "| yes| 1460|\n",
            "+----+-----+\n",
            "\n",
            "+---------+-----+\n",
            "|  contact|count|\n",
            "+---------+-----+\n",
            "|  unknown| 2346|\n",
            "| cellular| 8042|\n",
            "|telephone|  774|\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.groupBy(\"default\").count().show()\n",
        "df.groupBy(\"loan\").count().show()\n",
        "df.groupBy(\"contact\").count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af6nDWN_d6sv"
      },
      "source": [
        "### Q2. Correlation matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtcvanUNd6sw",
        "outputId": "2e6c7a5b-afcc-49a0-bc73-7446720cd1b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- job: string (nullable = true)\n",
            " |-- marital: string (nullable = true)\n",
            " |-- education: string (nullable = true)\n",
            " |-- housing: string (nullable = true)\n",
            " |-- deposit: string (nullable = true)\n",
            " |-- default: string (nullable = true)\n",
            " |-- loan: string (nullable = true)\n",
            " |-- contact: string (nullable = true)\n",
            " |-- features: vector (nullable = true)\n",
            "\n",
            "Correlation matrix :\n",
            "DenseMatrix([[ 1.00000000e+00,  1.12299889e-01,  1.89228074e-04,\n",
            "              -5.27793616e-03,  2.77383431e-03,  2.01685612e-02],\n",
            "             [ 1.12299889e-01,  1.00000000e+00,  2.24361313e-02,\n",
            "              -1.38938225e-02,  1.74111486e-02,  3.08052469e-02],\n",
            "             [ 1.89228074e-04,  2.24361313e-02,  1.00000000e+00,\n",
            "              -4.15574588e-02, -2.73915532e-02, -2.67161713e-02],\n",
            "             [-5.27793616e-03, -1.38938225e-02, -4.15574588e-02,\n",
            "               1.00000000e+00, -1.02726048e-01, -4.96994980e-02],\n",
            "             [ 2.77383431e-03,  1.74111486e-02, -2.73915532e-02,\n",
            "              -1.02726048e-01,  1.00000000e+00,  5.07271588e-01],\n",
            "             [ 2.01685612e-02,  3.08052469e-02, -2.67161713e-02,\n",
            "              -4.96994980e-02,  5.07271588e-01,  1.00000000e+00]])\n",
            "Time elapsed: 43.138899087905884\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>balance</th>\n",
              "      <th>duration</th>\n",
              "      <th>campaign</th>\n",
              "      <th>pdays</th>\n",
              "      <th>previous</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>age</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.112300</td>\n",
              "      <td>0.000189</td>\n",
              "      <td>-0.005278</td>\n",
              "      <td>0.002774</td>\n",
              "      <td>0.020169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>balance</th>\n",
              "      <td>0.112300</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.022436</td>\n",
              "      <td>-0.013894</td>\n",
              "      <td>0.017411</td>\n",
              "      <td>0.030805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>duration</th>\n",
              "      <td>0.000189</td>\n",
              "      <td>0.022436</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.041557</td>\n",
              "      <td>-0.027392</td>\n",
              "      <td>-0.026716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>campaign</th>\n",
              "      <td>-0.005278</td>\n",
              "      <td>-0.013894</td>\n",
              "      <td>-0.041557</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.102726</td>\n",
              "      <td>-0.049699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pdays</th>\n",
              "      <td>0.002774</td>\n",
              "      <td>0.017411</td>\n",
              "      <td>-0.027392</td>\n",
              "      <td>-0.102726</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.507272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>previous</th>\n",
              "      <td>0.020169</td>\n",
              "      <td>0.030805</td>\n",
              "      <td>-0.026716</td>\n",
              "      <td>-0.049699</td>\n",
              "      <td>0.507272</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               age   balance  duration  campaign     pdays  previous\n",
              "age       1.000000  0.112300  0.000189 -0.005278  0.002774  0.020169\n",
              "balance   0.112300  1.000000  0.022436 -0.013894  0.017411  0.030805\n",
              "duration  0.000189  0.022436  1.000000 -0.041557 -0.027392 -0.026716\n",
              "campaign -0.005278 -0.013894 -0.041557  1.000000 -0.102726 -0.049699\n",
              "pdays     0.002774  0.017411 -0.027392 -0.102726  1.000000  0.507272\n",
              "previous  0.020169  0.030805 -0.026716 -0.049699  0.507272  1.000000"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Prepare dataset for ML operations\n",
        "\n",
        "qual_vars = ['job', 'marital', 'education', 'housing', 'deposit', 'default', 'loan', 'contact'] \n",
        "quan_vars = ['age','balance','duration','campaign','pdays','previous']\n",
        "\n",
        "# put quantitative variables in a Vector of features\n",
        "assembler = VectorAssembler(inputCols=quan_vars, outputCol = 'features')\n",
        "input_dat = assembler.transform(df).select(qual_vars + ['features'])\n",
        "input_dat.printSchema()\n",
        "\n",
        "# Correlation matrix\n",
        "pearsonCorr = Correlation.corr(input_dat, 'features', 'pearson').collect()[0][0]\n",
        "print('Correlation matrix :')\n",
        "print(pearsonCorr)\n",
        "print('Time elapsed: {}'.format(time.time()-start))\n",
        "\n",
        "#pandas visualisation\n",
        "df_corr = pd.DataFrame(pearsonCorr.toArray(), columns = quan_vars, index = quan_vars)\n",
        "df_corr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hThfla5kD9l"
      },
      "source": [
        "### Q3. Feature selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFhTMgM8d6sx",
        "outputId": "386c46e9-324b-4255-b10b-ec06ba9493b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+--------+---------+-------+-------+-------+----+-------+--------------------+\n",
            "|        job| marital|education|housing|deposit|default|loan|contact|      scaledFeatures|\n",
            "+-----------+--------+---------+-------+-------+-------+----+-------+--------------------+\n",
            "|     admin.| married|secondary|    yes|    yes|     no|  no|unknown|[17.7680523203725...|\n",
            "|     admin.| married|secondary|     no|    yes|     no|  no|unknown|[14.7680523203725...|\n",
            "| technician| married|secondary|    yes|    yes|     no|  no|unknown|[-0.2319476796274...|\n",
            "|   services| married|secondary|    yes|    yes|     no|  no|unknown|[13.7680523203725...|\n",
            "|     admin.| married| tertiary|     no|    yes|     no|  no|unknown|[12.7680523203725...|\n",
            "| management|  single| tertiary|    yes|    yes|     no| yes|unknown|[0.76805232037259...|\n",
            "| management| married| tertiary|    yes|    yes|     no| yes|unknown|[14.7680523203725...|\n",
            "|    retired|divorced|secondary|    yes|    yes|     no|  no|unknown|[18.7680523203725...|\n",
            "| technician| married|secondary|    yes|    yes|     no|  no|unknown|[-4.2319476796274...|\n",
            "|   services|  single|secondary|    yes|    yes|     no|  no|unknown|[-13.231947679627...|\n",
            "|     admin.|  single|secondary|    yes|    yes|     no|  no|unknown|[-3.2319476796274...|\n",
            "|blue-collar| married|secondary|    yes|    yes|     no|  no|unknown|[-11.231947679627...|\n",
            "| management| married| tertiary|    yes|    yes|     no| yes|unknown|[-12.231947679627...|\n",
            "|blue-collar|  single| tertiary|    yes|    yes|     no|  no|unknown|[4.76805232037259...|\n",
            "| technician|  single| tertiary|    yes|    yes|     no|  no|unknown|[-10.231947679627...|\n",
            "| management|divorced| tertiary|    yes|    yes|     no|  no|unknown|[-6.2319476796274...|\n",
            "|blue-collar|  single|  primary|    yes|    yes|     no|  no|unknown|[-9.2319476796274...|\n",
            "|   services| married|secondary|    yes|    yes|     no|  no|unknown|[7.76805232037259...|\n",
            "|     admin.| married|secondary|    yes|    yes|     no|  no|unknown|[-0.2319476796274...|\n",
            "|     admin.|divorced|secondary|    yes|    yes|     no| yes|unknown|[7.76805232037259...|\n",
            "+-----------+--------+---------+-------+-------+-------+----+-------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Feature selection\n",
        "start = time.time()\n",
        "\n",
        "# center and normalise column-wise before applying PCA\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=False, withMean=True)\n",
        "\n",
        "# Compute summary statistics by fitting the StandardScaler\n",
        "scalerModel = scaler.fit(input_dat)\n",
        "\n",
        "# Normalize each feature to have unit standard deviation.\n",
        "scaledData = scalerModel.transform(input_dat)\n",
        "scaledData.select(qual_vars + ['scaledFeatures']).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2OenXMRd6sy"
      },
      "outputs": [],
      "source": [
        "# Apply PCA\n",
        "\n",
        "pca = PCA(k=3, inputCol = scaler.getOutputCol(), outputCol=\"pcaFeatures\")\n",
        "model = pca.fit(scaledData)\n",
        "transformed_features = model.transform(scaledData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lf6VIEmXd6sz",
        "outputId": "c87cd807-1a98-4a1d-8e5f-7f90c30794f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Principal components:\n",
            "DenseMatrix([[-4.14792301e-04,  8.11977961e-05,  8.30434367e-05],\n",
            "             [-9.99996757e-01,  2.43703351e-03, -6.11336625e-04],\n",
            "             [-2.44285482e-03, -9.99950324e-01,  9.65648450e-03],\n",
            "             [ 1.17364875e-05,  3.21116157e-04, -2.60118942e-03],\n",
            "             [-5.87509530e-04,  9.65731689e-03,  9.99892808e-01],\n",
            "             [-2.18924525e-05,  1.91221157e-04,  1.06760443e-02]])\n",
            "+--------------------+\n",
            "|         pcaFeatures|\n",
            "+--------------------+\n",
            "|[-816.07218813510...|\n",
            "|[1480.88339123302...|\n",
            "|[256.084128093277...|\n",
            "|[-947.93905590571...|\n",
            "|[1343.82431001608...|\n",
            "+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Explained variance: [0.987431930087071,0.011432185128056245,0.001121519333585062]\n",
            "Time elapsed: 3.32\n"
          ]
        }
      ],
      "source": [
        "# Principal components and explained variance\n",
        "\n",
        "print('Principal components:')\n",
        "print(model.pc) # principal components\n",
        "transformed_features.select('pcaFeatures').show(5)\n",
        "print('Explained variance: {}'.format(model.explainedVariance)) # explained variance\n",
        "print('Time elapsed:',np.round(time.time()-start,2))\n",
        "\n",
        "#pandas and numpy visualisations\n",
        "#from pyspark.sql import Row\n",
        "#pcs = np.round(model.pc.toArray(),4)\n",
        "#df_pc = pd.DataFrame(pcs, columns = ['PC1','PC2','PC3'], index = df.columns[2:])\n",
        "#df_pc\n",
        "#transformed_features.select('pcaFeatures').rdd.map(lambda x: Row(pcaFeatures = str([x for x in list(np.round(x[0].toArray(),4))]))).toDF().show(5,truncate = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98Hss2Dvd6s0"
      },
      "source": [
        "### Q4. Linear regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WH-n7R5Pj-To",
        "outputId": "4f1be7bc-ff00-49c9-d6aa-52401f24b977"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------+\n",
            "|pdays|features|\n",
            "+-----+--------+\n",
            "|-1   |[0.0]   |\n",
            "|-1   |[0.0]   |\n",
            "|-1   |[0.0]   |\n",
            "|-1   |[0.0]   |\n",
            "|-1   |[0.0]   |\n",
            "+-----+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Build a new assembled dataset, try to predict value of 'pdays' variable using the 'previous' variable\n",
        "# note: corr(pday,previous) = 0.5, the only significant one, so it's worth trying this setting\n",
        "\n",
        "start = time.time()\n",
        "#reg_vars = ['age','balance','duration','campaign','previous']\n",
        "assembler = VectorAssembler(inputCols=['previous'], outputCol = 'features')\n",
        "input_reg = assembler.transform(df).select('pdays', 'features')\n",
        "input_reg.show(5, truncate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtnfDbMid6s1",
        "outputId": "0e807ca9-3d4f-404a-aa44-4d517528b937"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10056 1106\n"
          ]
        }
      ],
      "source": [
        "# split training and test sets\n",
        "\n",
        "splits = input_reg.randomSplit([0.9, 0.1])\n",
        "train_df = splits[0]\n",
        "test_df = splits[1]\n",
        "print(train_df.count(),test_df.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mk0heIAfd6s2",
        "outputId": "e6431f2f-d97f-4260-fe3d-e8e76042a326"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coefficients: [23.551551677403662]\n",
            "Intercept: 31.239931851804332\n"
          ]
        }
      ],
      "source": [
        "# perform linear regression for variable 'pdays'\n",
        "\n",
        "lr = LinearRegression(featuresCol='features', labelCol='pdays', maxIter=100, regParam=0.00001, elasticNetParam=0.85)\n",
        "lr_model = lr.fit(train_df)\n",
        "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
        "print(\"Intercept: \" + str(lr_model.intercept))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EWQkmE-d6s2",
        "outputId": "b57ef7c4-14ca-46e9-ebc3-676803c9eeca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE: 93.148806\n",
            "R2: 0.253946\n"
          ]
        }
      ],
      "source": [
        "# summary of results on training data\n",
        "\n",
        "trainingSummary = lr_model.summary\n",
        "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
        "print(\"R2: %f\" % trainingSummary.r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ExwBD98d6s3",
        "outputId": "8f119620-3562-4f8e-a7d2-210e5da1aa91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE: 98.8671\n",
            "R2: 0.28124\n"
          ]
        }
      ],
      "source": [
        "# summary of results on test data\n",
        "\n",
        "lr_predictions = lr_model.transform(test_df)\n",
        "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"pdays\", metricName=\"r2\")\n",
        "test_result = lr_model.evaluate(test_df)\n",
        "print(\"RMSE: %g\" % test_result.rootMeanSquaredError)\n",
        "print(\"R2: %g\" % lr_evaluator.evaluate(lr_predictions))\n",
        "# NOTE: R2 and RMSE are not very good as a result of all variables being poorly correlated in the first place."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BarEHU0bd6s3",
        "outputId": "87f0030f-ca9a-42bf-d06c-e6901f5ae838"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+-----+--------+\n",
            "|        prediction|pdays|features|\n",
            "+------------------+-----+--------+\n",
            "|31.239931851804332|   -1|   [0.0]|\n",
            "|31.239931851804332|   -1|   [0.0]|\n",
            "|31.239931851804332|   -1|   [0.0]|\n",
            "|31.239931851804332|   -1|   [0.0]|\n",
            "|31.239931851804332|   -1|   [0.0]|\n",
            "+------------------+-----+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "predictions = lr_model.transform(test_df)\n",
        "predictions.select(\"prediction\",\"pdays\",\"features\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDtwT1u5d6s4",
        "outputId": "e02d52f4-5865-4b9e-c274-a7e4953808e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time elapsed: 5.2130045890808105\n"
          ]
        }
      ],
      "source": [
        "print('Time elapsed:',(time.time()-start))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZX_o3yfkCHk"
      },
      "source": [
        "### Q5. Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhHNhP0Ad6s5",
        "outputId": "af7c995d-a5b9-4950-940d-ced580f2ed10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+---------------------------------+\n",
            "|label|features                         |\n",
            "+-----+---------------------------------+\n",
            "|0.0  |[59.0,2343.0,1042.0,1.0,-1.0,0.0]|\n",
            "|0.0  |[56.0,45.0,1467.0,1.0,-1.0,0.0]  |\n",
            "|0.0  |[41.0,1270.0,1389.0,1.0,-1.0,0.0]|\n",
            "|0.0  |[55.0,2476.0,579.0,1.0,-1.0,0.0] |\n",
            "|0.0  |[54.0,184.0,673.0,2.0,-1.0,0.0]  |\n",
            "+-----+---------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "10589 573\n"
          ]
        }
      ],
      "source": [
        "# Ternary classification of 'marital' variable (0-1-2) using all the other quantitative variables\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# turn target string variable to label format\n",
        "label_stringIdx = StringIndexer(inputCol = 'marital', outputCol = 'label')\n",
        "model = label_stringIdx.fit(df)\n",
        "df1 = model.transform(df)\n",
        "\n",
        "# turn quantitative variables to feature vector (all quan_vars)\n",
        "assembler = VectorAssembler(inputCols=quan_vars, outputCol = 'features')\n",
        "input_log = assembler.transform(df1).select('label', 'features')\n",
        "input_log.show(5, truncate = False)\n",
        "\n",
        "# split into training and test\n",
        "splits = input_log.randomSplit([0.95, 0.05])\n",
        "train_df = splits[0]\n",
        "test_df = splits[1]\n",
        "print(train_df.count(),test_df.count())\n",
        "\n",
        "# create and fit LR model\n",
        "log = LogisticRegression(featuresCol='features', labelCol='label', regParam=1e-4,\n",
        "                         maxIter=100, elasticNetParam=0.65, tol=1e-8)\n",
        "logModel = log.fit(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "em1kcDA_d6s5",
        "outputId": "b90a7ef4-ec99-492e-8780-29ce97c08662"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training, accuracy: 0.6642, area under ROC: 0.6522\n",
            "Test, accuracy: 0.6928, area under ROC: 0.6814\n",
            "Time elapsed: 19.11789298057556\n"
          ]
        }
      ],
      "source": [
        "# evaluate model\n",
        "\n",
        "# Create evaluators\n",
        "evaluatorMulti = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\") # for accuracy\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName='areaUnderROC') # for roc\n",
        "\n",
        "# Make predicitons\n",
        "out_tr = logModel.transform(train_df).select(\"label\", \"prediction\")\n",
        "out_te = logModel.transform(test_df).select(\"label\", \"prediction\")\n",
        "\n",
        "# Get metrics\n",
        "acc_tr = evaluatorMulti.evaluate(out_tr, {evaluatorMulti.metricName: \"accuracy\"})\n",
        "auc_tr = evaluator.evaluate(out_tr)\n",
        "acc_te = evaluatorMulti.evaluate(out_te, {evaluatorMulti.metricName: \"accuracy\"})\n",
        "auc_te = evaluator.evaluate(out_te)\n",
        "print('Training, accuracy: {:.4f}, area under ROC: {:.4f}'.format(acc_tr,auc_tr))\n",
        "print('Test, accuracy: {:.4f}, area under ROC: {:.4f}'.format(acc_te,auc_te))\n",
        "\n",
        "print('Time elapsed:',(time.time()-start)) # end time for cluster comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t-pKptyd6s6"
      },
      "source": [
        "### Q6. Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmsYTBgsd6s7",
        "outputId": "13c9f3cf-570b-4bd5-f6e2-9c0e7e9be4d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+---------------------------------+\n",
            "|label|features                         |\n",
            "+-----+---------------------------------+\n",
            "|1.0  |[59.0,2343.0,1042.0,1.0,-1.0,0.0]|\n",
            "|1.0  |[56.0,45.0,1467.0,1.0,-1.0,0.0]  |\n",
            "|1.0  |[41.0,1270.0,1389.0,1.0,-1.0,0.0]|\n",
            "|1.0  |[55.0,2476.0,579.0,1.0,-1.0,0.0] |\n",
            "|1.0  |[54.0,184.0,673.0,2.0,-1.0,0.0]  |\n",
            "+-----+---------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "10608 554\n"
          ]
        }
      ],
      "source": [
        "# Binary classification of 'deposit' variable using all the other quantitative variables\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# turn target string variable to label format\n",
        "label_stringIdx = StringIndexer(inputCol = 'deposit', outputCol = 'label')\n",
        "model = label_stringIdx.fit(df)\n",
        "df2 = model.transform(df)\n",
        "\n",
        "# turn quantitative variables to feature vector (all quan_vars)\n",
        "assembler = VectorAssembler(inputCols=quan_vars, outputCol = 'features')\n",
        "input_svm = assembler.transform(df2).select('label', 'features')\n",
        "input_svm.show(5, truncate = False)\n",
        "\n",
        "# split into training and test\n",
        "splits = input_svm.randomSplit([0.95, 0.05])\n",
        "train_df = splits[0]\n",
        "test_df = splits[1]\n",
        "print(train_df.count(),test_df.count())\n",
        "\n",
        "svm = LinearSVC(featuresCol='features', labelCol='label', regParam=0.1, maxIter=200)\n",
        "svmModel = svm.fit(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRruS4PSd6s7",
        "outputId": "61c0d7ba-ade8-4307-97bd-a4338a67f5c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training, accuracy: 0.7328, area under ROC: 0.7264\n",
            "Test, accuracy: 0.7310, area under ROC: 0.7293\n",
            "Time elapsed: 12.639104843139648\n"
          ]
        }
      ],
      "source": [
        "# evaluate model\n",
        "\n",
        "# Create evaluators\n",
        "evaluatorMulti = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\") # for accuracy\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName='areaUnderROC') # for roc\n",
        "\n",
        "# Make predicitons\n",
        "out_tr = svmModel.transform(train_df).select(\"label\", \"prediction\")\n",
        "out_te = svmModel.transform(test_df).select(\"label\", \"prediction\")\n",
        "\n",
        "# Get metrics\n",
        "acc_tr = evaluatorMulti.evaluate(out_tr, {evaluatorMulti.metricName: \"accuracy\"})\n",
        "auc_tr = evaluator.evaluate(out_tr)\n",
        "acc_te = evaluatorMulti.evaluate(out_te, {evaluatorMulti.metricName: \"accuracy\"})\n",
        "auc_te = evaluator.evaluate(out_te)\n",
        "\n",
        "print('Training, accuracy: {:.4f}, area under ROC: {:.4f}'.format(acc_tr,auc_tr))\n",
        "print('Test, accuracy: {:.4f}, area under ROC: {:.4f}'.format(acc_te,auc_te))\n",
        "\n",
        "print('Time elapsed:',(time.time()-start)) # end time for cluster comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnGTGwDpkONB"
      },
      "source": [
        "### Q7. K-Means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guYxfXYad6s8",
        "outputId": "d42cd254-54dd-467f-bf4a-babce1a7ecf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "k = 2, silhouette score: 0.9398805175144744\n",
            "k = 3, silhouette score: 0.8777665380478766\n",
            "k = 4, silhouette score: 0.8278052038397186\n",
            "k = 5, silhouette score: 0.8234949531297522\n",
            "k = 6, silhouette score: 0.7725640000849053\n",
            "k = 7, silhouette score: 0.7202539616957179\n",
            "k = 8, silhouette score: 0.7310444194004343\n",
            "k = 9, silhouette score: 0.6110342465304519\n"
          ]
        }
      ],
      "source": [
        "# K-means clustering of records by quantitative variables\n",
        "\n",
        "start = time.time()\n",
        "silhouette_score=[]\n",
        "evaluator = ClusteringEvaluator(predictionCol='prediction', featuresCol='features', \\\n",
        "                                metricName='silhouette', distanceMeasure='squaredEuclidean')\n",
        "\n",
        "for i in range(2,10):\n",
        "    kmeans = KMeans(featuresCol='features', k=i)\n",
        "    model_km = kmeans.fit(input_dat)\n",
        "    output = model_km.transform(input_dat)\n",
        "    score = evaluator.evaluate(output)\n",
        "    silhouette_score.append(score)\n",
        "    print(\"k = {}, silhouette score: {}\".format(i,score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKW9B1ejd6s9",
        "outputId": "1f4ebad8-6a82-4435-d490-b80f05f9aff2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time elapsed: (26.960657119750977, 2)\n"
          ]
        }
      ],
      "source": [
        "print('Time elapsed:',(time.time()-start,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk3OcH3dd6s-"
      },
      "source": [
        "### Q8. Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWXJCsC3d6s-",
        "outputId": "e2dc10bf-82cd-4d29-c2c0-8d1bbb260e1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+---------------------------------+\n",
            "|label|features                         |\n",
            "+-----+---------------------------------+\n",
            "|0.0  |[59.0,2343.0,1042.0,1.0,-1.0,0.0]|\n",
            "|0.0  |[56.0,45.0,1467.0,1.0,-1.0,0.0]  |\n",
            "|0.0  |[41.0,1270.0,1389.0,1.0,-1.0,0.0]|\n",
            "|0.0  |[55.0,2476.0,579.0,1.0,-1.0,0.0] |\n",
            "|0.0  |[54.0,184.0,673.0,2.0,-1.0,0.0]  |\n",
            "+-----+---------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "10606 556\n"
          ]
        }
      ],
      "source": [
        "# Example with logistic regression on variable 'marital'\n",
        "\n",
        "input_log.show(5, truncate = False)\n",
        "\n",
        "# split into training and test\n",
        "splits = input_log.randomSplit([0.95, 0.05])\n",
        "train_df = splits[0]\n",
        "test_df = splits[1]\n",
        "print(train_df.count(),test_df.count())\n",
        "\n",
        "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
        "\n",
        "# Create ParamGrid for Cross Validation\n",
        "lrparamGrid = (ParamGridBuilder()\n",
        "             .addGrid(lr.regParam, [0.0001, 0.001, 0.01, 0.1])\n",
        "             #  .addGrid(lr.regParam, [0.01, 0.1, 0.5])\n",
        "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
        "             #  .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
        "             .build())\n",
        "\n",
        "# Evaluate model\n",
        "lrevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\") # for accuracy\n",
        "\n",
        "# Create 5-fold CrossValidator\n",
        "lrcv = CrossValidator(estimator = lr,\n",
        "                    estimatorParamMaps = lrparamGrid,\n",
        "                    evaluator = lrevaluator,\n",
        "                    numFolds = 5)\n",
        "\n",
        "# Run cross validations\n",
        "lrcvModel = lrcv.fit(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1--qEFWSd6s-",
        "outputId": "e8084012-1cf8-4bfc-d165-b8cc9c8738e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training, accuracy: 0.6229, area under ROC: 0.6542\n",
            "Test, accuracy: 0.6133, area under ROC: 0.6388\n",
            "Time elapsed: 468.4383044242859\n"
          ]
        }
      ],
      "source": [
        "# Create evaluator for area under roc (not optimised)\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName='areaUnderROC') # for roc\n",
        "\n",
        "# Make predicitons using best model\n",
        "out_tr = lrcvModel.bestModel.transform(train_df).select(\"label\", \"prediction\")\n",
        "out_te = lrcvModel.bestModel.transform(test_df).select(\"label\", \"prediction\")\n",
        "\n",
        "# Get metrics\n",
        "acc_tr = lrevaluator.evaluate(out_tr, {evaluatorMulti.metricName: \"accuracy\"})\n",
        "auc_tr = evaluator.evaluate(out_tr)\n",
        "acc_te = lrevaluator.evaluate(out_te, {evaluatorMulti.metricName: \"accuracy\"})\n",
        "auc_te = evaluator.evaluate(out_te)\n",
        "\n",
        "print('Training, accuracy: {:.4f}, area under ROC: {:.4f}'.format(acc_tr,auc_tr))\n",
        "print('Test, accuracy: {:.4f}, area under ROC: {:.4f}'.format(acc_te,auc_te))\n",
        "print('Time elapsed:',(time.time()-start))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Giampaoli_Daniele_LSC_Final_Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}